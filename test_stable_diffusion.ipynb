{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text2Image flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "device = \"cuda\"\n",
    "\n",
    "# this for the regular FP32 model\n",
    "#pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True)\n",
    "#pipe = pipe.to(device)\n",
    "\n",
    "#this for the FP16 model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision=\"fp16\", use_auth_token=True)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "batch_size = 1\n",
    "prompt_input = [prompt] * batch_size\n",
    "\n",
    "with autocast(\"cuda\"):\n",
    "    image = pipe(prompt_input, guidance_scale=7.5).images[0]\n",
    "    \n",
    "#image.save(\"astronaut_rides_horse.png\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to test out how to convert to base64, to pass in using to the managed endpoint\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "result_image_str = [\"\"] * 3\n",
    "print(len(result_image_str))\n",
    "\n",
    "for i in range(len(images)):\n",
    "\n",
    "    buffered_result_image = BytesIO()\n",
    "\n",
    "    images[i].save(buffered_result_image, format=\"PNG\")\n",
    "    result_image_str[i] = base64.b64encode(buffered_result_image.getvalue()).decode('utf-8')\n",
    "\n",
    "result_image_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how you save the model to register on the aml workspace\n",
    "pipe.save_pretrained('models/stable-diffusion-v1-5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image2Image Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "text_encoder/model.safetensors not found\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "/anaconda/envs/sd_xl/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "#model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "#this for the FP16 model\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision=\"fp16\", use_auth_token=True)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "file =  os.path.join(\"images\", \"elmo.jpg\")\n",
    "img = Image.open(file).convert(\"RGB\")\n",
    "img = img.resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of elmo stangind in front of a fiery pit\"\n",
    "\n",
    "with autocast(\"cuda\"):\n",
    "    image = pipe(prompt=prompt, image=img, strength=.75, guidance_scale=7.5).images[0]\n",
    "    \n",
    "image\n",
    "\n",
    "#image.save(\"astronaut_rides_horse.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a collage of 3 by 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "file =  os.path.join(\"images\", \"elmo.jpg\")\n",
    "img = Image.open(file).convert(\"RGB\")\n",
    "img = img.resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cedccf5038e4e11841b89ba69b97800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebeb3bb37e93402dbd302e51042749f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d723d035050841058bb821ea39e27766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842da83d42f94c729da4074b0bce3e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b967611154945fcb08d47d3cca17156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d172ed58d72e4e049795528debd95187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596c09f19abe48d9a069f1c34eb6dffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df59641bdb445298ec39b4a797e05b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f4d00be1d04ce69e7e7029ab83dc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cc0864efca41bc8fe9754a27089d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db17f6c68534c55bc13dceec82e25b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2832326a656447f8ebc3a7f08855bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800f822e81b4450fbda2feab546621da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e287989e327e4f47a30c61bc8b980f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bda121c63c54af489dfeb37516f7c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = 3\n",
    "columns =5\n",
    "\n",
    "actual_prompt = \"A painting of elmo raising his arms standing in front of a fire\"\n",
    "\n",
    "prompts = [\"{0} in the style of Greg Rutkowski\",\n",
    "           \"{0} in the style of Salvador Dali\",\n",
    "           \"{0} in the style of Japanese Anime\",\n",
    "           \"{0} in the style of Vincent Van Gogh\",\n",
    "           \"{0} in the style of Rembrandt\",\n",
    "           \"{0} in the style of Edgar Degas\",\n",
    "           \"{0} in the style of Pablo Picasso\",\n",
    "           \"{0} in the style of Andy Wharhol\",\n",
    "           \"{0} in the style of Georgia O'Keefe\",\n",
    "           \"{0} in the style of Monet\",\n",
    "           \"{0} in the style of Vermeer\",\n",
    "           \"{0} in the style of Edvard Munch\",\n",
    "           \"{0} in the style of Henri Matisse\",\n",
    "           \"{0} in the style of Gustav Klimt\",\n",
    "           \"{0} in the style of Edward Hopper\"]\n",
    "\n",
    "images = [\"\"] * (rows*columns)\n",
    "\n",
    "#for some reason it's bumping into the nsfw filter, nothing seems innapropiate so disabiling it\n",
    "def dummy(images, **kwargs):\n",
    "    return (images, [False])\n",
    "pipe.safety_checker = dummy\n",
    "\n",
    "for i in range(rows*columns):\n",
    "    images[i] = pipe(prompt=prompts[i].format(actual_prompt), image=img, strength=.75, guidance_scale=7.5).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT wrote this\n",
    "def create_collage(images, output_path, num_columns, num_rows):\n",
    "    total_images = num_columns * num_rows\n",
    "\n",
    "    # Check if the number of images matches the specified grid\n",
    "    if len(images) != total_images:\n",
    "        raise ValueError(\"Number of images doesn't match the grid size.\")\n",
    "\n",
    "    # Get the size of the first image\n",
    "    image_width, image_height = images[0].size\n",
    "\n",
    "    # Calculate collage size\n",
    "    collage_width = num_columns * image_width\n",
    "    collage_height = num_rows * image_height\n",
    "\n",
    "    # Create a new blank image for the collage\n",
    "    collage = Image.new('RGB', (collage_width, collage_height))\n",
    "\n",
    "    # Paste each image onto the collage\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_columns):\n",
    "            idx = i * num_columns + j\n",
    "            collage.paste(images[idx], (j * image_width, i * image_height))\n",
    "\n",
    "    # Save the collage\n",
    "    collage.save(output_path)\n",
    "\n",
    "\n",
    "create_collage(images, \"images/collage_elmo.jpg\", columns, rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ldm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "836061b101677ab67408faaca0974c9fbb278c3803e5a934441e06f11cb6d61b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
